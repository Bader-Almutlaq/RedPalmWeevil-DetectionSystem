{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palm Weevil Trap Detection - Model Evaluation\n",
    "\n",
    "This notebook evaluates deep learning models trained to classify between Red Palm Weevil (RPW) traps and Non-RPW traps from images. We'll evaluate two models:\n",
    "1. MobileNetV3 Large\n",
    "2. EfficientNet B0\n",
    "\n",
    "## Overview\n",
    "- Load pre-trained models\n",
    "- Process test images with the same transformations used during training\n",
    "- Evaluate model performance on test data\n",
    "- Compare results between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import (\n",
    "    mobilenet_v3_large,\n",
    "    MobileNet_V3_Large_Weights,\n",
    "    efficientnet_b0,\n",
    "    EfficientNet_B0_Weights,\n",
    "    efficientnet_b2,\n",
    "    EfficientNet_B2_Weights,\n",
    "    efficientnet_b3,\n",
    "    EfficientNet_B3_Weights,\n",
    "    efficientnet_b4,\n",
    "    EfficientNet_B4_Weights,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Device setup - use GPU if available\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "    \"mobilenet\": {\n",
    "        \"builder\": mobilenet_v3_large,\n",
    "        \"weights\": MobileNet_V3_Large_Weights.DEFAULT,\n",
    "        \"classifier_index\": 0,\n",
    "        \"input_size\": 224,\n",
    "    },\n",
    "    \"efficientnet_b0\": {\n",
    "        \"builder\": efficientnet_b0,\n",
    "        \"weights\": EfficientNet_B0_Weights.DEFAULT,\n",
    "        \"classifier_index\": 1,\n",
    "        \"input_size\": 224,\n",
    "    },\n",
    "    \"efficientnet_b2\": {\n",
    "        \"builder\": efficientnet_b2,\n",
    "        \"weights\": EfficientNet_B2_Weights.DEFAULT,\n",
    "        \"classifier_index\": 1,\n",
    "        \"input_size\": 260,\n",
    "    },\n",
    "    \"efficientnet_b3\": {\n",
    "        \"builder\": efficientnet_b3,\n",
    "        \"weights\": EfficientNet_B3_Weights.DEFAULT,\n",
    "        \"classifier_index\": 1,\n",
    "        \"input_size\": 300,\n",
    "    },\n",
    "    \"efficientnet_b4\": {\n",
    "        \"builder\": efficientnet_b4,\n",
    "        \"weights\": EfficientNet_B4_Weights.DEFAULT,\n",
    "        \"classifier_index\": 1,\n",
    "        \"input_size\": 380,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Function\n",
    "\n",
    "This function loads our trained models from saved checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, path, num_classes=2):\n",
    "    if model_name not in MODEL_CONFIGS:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    model = config[\"builder\"](\n",
    "        weights=None\n",
    "    )  # Initialize the model without pre-trained weights\n",
    "\n",
    "    # Access the correct layer's in_features depending on the model\n",
    "    in_features = model.classifier[config[\"classifier_index\"]].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes),\n",
    "    )\n",
    "\n",
    "    # Load the trained weights from the path\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model.to(device)  # Move the model to the correct device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "We apply the same transformations that were used during model training:\n",
    "1. Resize images based on the model\n",
    "2. Convert to PyTorch tensors\n",
    "3. Normalize using ImageNet mean and standard deviation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(model_name):\n",
    "    if model_name not in MODEL_CONFIGS:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "\n",
    "    model_config = MODEL_CONFIGS[model_name]\n",
    "    img_size = model_config[\"input_size\"]\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Function\n",
    "\n",
    "This function loads test images from their respective class folders and processes them using our transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_trap_images(folder_path, class_map):\n",
    "    images, labels = [], []\n",
    "\n",
    "    for class_name, label in class_map.items():\n",
    "        class_folder = os.path.join(folder_path, class_name)\n",
    "        img_paths = glob.glob(os.path.join(class_folder, \"*.*\"))\n",
    "        if not img_paths:\n",
    "            print(f\"⚠️ Skipping {class_name} — no images found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Found {len(img_paths)} images in class {class_name}\")\n",
    "        for img_path in img_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")  # No transform here\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "\n",
    "    if not images:\n",
    "        return None, None\n",
    "\n",
    "    # Change this line to return raw images and labels (no transformation here)\n",
    "    return images, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "\n",
    "This function evaluates model performance using:\n",
    "1. Accuracy metric\n",
    "2. Confusion matrix visualization\n",
    "\n",
    "The confusion matrix helps us understand false positives and false negatives in our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, class_names=None, show_cm=True):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device, non_blocking=(device.type == \"cuda\"))\n",
    "            labels = labels.to(device, non_blocking=(device.type == \"cuda\"))\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Infer class names if not provided\n",
    "    if (\n",
    "        class_names is None\n",
    "        and hasattr(test_loader.dataset, \"dataset\")\n",
    "        and hasattr(test_loader.dataset.dataset, \"classes\")\n",
    "    ):\n",
    "        class_names = test_loader.dataset.dataset.classes\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            all_labels, all_preds, target_names=class_names, zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Confusion matrix\n",
    "    if show_cm:\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "        )\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Evaluation\n",
    "\n",
    "Now we'll load the test data and evaluate both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definitions\n",
    "class_names = [\"NRPW-trap\", \"RPW-crop-augmented\"]\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "# Load test data\n",
    "test_root = \"./test\"\n",
    "print(f\"Loading test images from {test_root}...\")\n",
    "raw_images, raw_labels = load_raw_trap_images(test_root, class_to_idx)\n",
    "\n",
    "if raw_images is None or len(raw_images) == 0:\n",
    "    print(\"No images to evaluate. Please check your test data directory.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(raw_images)} test images in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Custom Dataset class\n",
    "class TrapImagesDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Apply transformation and create DataLoader\n",
    "def create_dataloader(images, labels, model_name, batch_size=32):\n",
    "    transform = get_transform(model_name)\n",
    "    dataset = TrapImagesDataset(images, labels, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3 Evaluation\n",
    "\n",
    "MobileNetV3 is designed to be computationally efficient while maintaining good accuracy. Let's evaluate its performance on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the model (MobileNetV3)\n",
    "    print(\"\\n🔍 Loading MobileNetV3 model...\")\n",
    "    mobilenet = load_model(\"mobilenet\", \"saved_models_states/mobilenetv3_rpw.pth\")\n",
    "    print(\"MobileNetV3 model loaded successfully.\")\n",
    "\n",
    "    # Apply transformation and create DataLoader\n",
    "    test_loader = create_dataloader(raw_images, raw_labels, \"mobilenet\", batch_size=32)\n",
    "\n",
    "    # Now, pass the DataLoader to the evaluation function\n",
    "    mobile_metrics = evaluate(mobilenet, test_loader, class_names)\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating MobileNetV3: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetB0 Evaluation\n",
    "\n",
    "EfficientNetB0 is known for its scaling method that uniformly scales network width, depth, and resolution. Let's evaluate EfficientNet B0 on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the model (EfficientNetB0)\n",
    "    print(\"\\n🔍 Loading EfficientNetB0 model...\")\n",
    "    efficientnet = load_model(\n",
    "        \"efficientnet_b0\", \"saved_models_states/efficientnet_b0_rpw.pth\"\n",
    "    )\n",
    "    print(\"EfficientNetB0 model loaded successfully.\")\n",
    "\n",
    "    # Apply transformation and create DataLoader\n",
    "    test_loader = create_dataloader(\n",
    "        raw_images, raw_labels, \"efficientnet_b0\", batch_size=32\n",
    "    )\n",
    "\n",
    "    # Now, pass the DataLoader to the evaluation function\n",
    "    eff0_metrics = evaluate(efficientnet, test_loader, class_names)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating EfficientNetB0: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetB2 Evaluation\n",
    "\n",
    "EfficientNetB2 is known for its scaling method that uniformly scales network width, depth, and resolution. Let's evaluate EfficientNet B2 on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the model (EfficientNetB2)\n",
    "    print(\"\\n🔍 Loading EfficientNetB2 model...\")\n",
    "    efficientnet = load_model(\n",
    "        \"efficientnet_b2\", \"saved_models_states/efficientnet_b2_rpw.pth\"\n",
    "    )\n",
    "    print(\"EfficientNetB2 model loaded successfully.\")\n",
    "\n",
    "    # Apply transformation and create DataLoader\n",
    "    test_loader = create_dataloader(\n",
    "        raw_images, raw_labels, \"efficientnet_b2\", batch_size=32\n",
    "    )\n",
    "\n",
    "    # Now, pass the DataLoader to the evaluation function\n",
    "    eff2_metrics = evaluate(efficientnet, test_loader, class_names)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating EfficientNetB2: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetB3 Evaluation\n",
    "\n",
    "EfficientNetB3 is known for its scaling method that uniformly scales network width, depth, and resolution. Let's evaluate EfficientNet B3 on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the model (EfficientNetB3)\n",
    "    print(\"\\n🔍 Loading EfficientNetB3 model...\")\n",
    "    efficientnet = load_model(\n",
    "        \"efficientnet_b3\", \"saved_models_states/efficientnet_b3_rpw.pth\"\n",
    "    )\n",
    "    print(\"EfficientNetB3 model loaded successfully.\")\n",
    "\n",
    "    # Apply transformation and create DataLoader\n",
    "    test_loader = create_dataloader(\n",
    "        raw_images, raw_labels, \"efficientnet_b3\", batch_size=32\n",
    "    )\n",
    "\n",
    "    # Now, pass the DataLoader to the evaluation function\n",
    "    eff3_metrics = evaluate(efficientnet, test_loader, class_names)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating EfficientNetB3: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetB4 Evaluation\n",
    "\n",
    "EfficientNetB4 is known for its scaling method that uniformly scales network width, depth, and resolution. Let's evaluate EfficientNet B4 on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the model (EfficientNet4)\n",
    "    print(\"\\n🔍 Loading EfficientNetB4 model...\")\n",
    "    efficientnet = load_model(\n",
    "        \"efficientnet_b4\", \"saved_models_states/efficientnet_b4_rpw.pth\"\n",
    "    )\n",
    "    print(\"EfficientNetB4 model loaded successfully.\")\n",
    "\n",
    "    # Apply transformation and create DataLoader\n",
    "    test_loader = create_dataloader(\n",
    "        raw_images, raw_labels, \"efficientnet_b4\", batch_size=32\n",
    "    )\n",
    "\n",
    "    # Now, pass the DataLoader to the evaluation function§\n",
    "    eff4_metrics = evaluate(efficientnet, test_loader, class_names)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating EfficientNetB4: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's compare the performance of both models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Organize metrics into a dictionary\n",
    "model_metrics = {\n",
    "    \"MobileNetV3\": list(mobile_metrics),\n",
    "    \"EfficientNetB0\": list(eff0_metrics),\n",
    "    \"EfficientNetB2\": list(eff2_metrics),\n",
    "    \"EfficientNetB3\": list(eff3_metrics),\n",
    "    \"EfficientNetB4\": list(eff4_metrics),\n",
    "}\n",
    "\n",
    "\n",
    "# Step 3: Define plotting function\n",
    "def plot_model_comparison(model_metrics):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "    models = list(model_metrics.keys())\n",
    "    values = [model_metrics[m] for m in models]\n",
    "\n",
    "    num_metrics = len(metrics_names)\n",
    "    num_models = len(models)\n",
    "    x = np.arange(num_metrics)\n",
    "    width = 0.8 / num_models  # Keep bars within group\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Custom color palette\n",
    "    colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\", \"#9b59b6\", \"#1abc9c\"]\n",
    "\n",
    "    # Plot each model's bars\n",
    "    for i, (model_name, scores) in enumerate(model_metrics.items()):\n",
    "        offset = (i - num_models / 2) * width + width / 2\n",
    "        plt.bar(\n",
    "            x + offset,\n",
    "            scores,\n",
    "            width,\n",
    "            label=model_name,\n",
    "            color=colors[i % len(colors)],\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "        # Add value labels\n",
    "        for j, score in enumerate(scores):\n",
    "            plt.text(\n",
    "                x[j] + offset,\n",
    "                score - 0.03,\n",
    "                f\"{score:.3f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"top\",\n",
    "                fontsize=10,\n",
    "                fontweight=\"bold\",\n",
    "                color=\"white\",\n",
    "            )\n",
    "\n",
    "    # Styling\n",
    "    plt.xticks(x, metrics_names, fontsize=12, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Score\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.title(\"Model Performance Comparison\", fontsize=18, fontweight=\"bold\", pad=20)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.ylim(max(0, min(min(v) for v in values) - 0.05), 1.0)\n",
    "    plt.gca().set_facecolor(\"#f8f9fa\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Step 4: Plot all together\n",
    "plot_model_comparison(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we evaluated two deep learning models on a palm weevil trap classification task. The evaluation metrics include:\n",
    "\n",
    "1. Overall accuracy\n",
    "2. Confusion matrix showing true positives, true negatives, false positives, and false negatives\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Based on the evaluation results, you might consider:\n",
    "- Using the better-performing model for deployment\n",
    "- Collecting more training data if accuracy is not satisfactory\n",
    "- Trying different model architectures or hyperparameters\n",
    "- Implementing ensemble methods to combine predictions from both models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
