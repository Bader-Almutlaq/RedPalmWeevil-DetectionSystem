{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palm Weevil Trap Detection - Model Evaluation\n",
    "\n",
    "This notebook evaluates deep learning models trained to classify between Red Palm Weevil (RPW) traps and Non-RPW traps from images. We'll evaluate two models:\n",
    "1. MobileNetV3 Large\n",
    "2. EfficientNet B0\n",
    "\n",
    "## Overview\n",
    "- Load pre-trained models\n",
    "- Process test images with the same transformations used during training\n",
    "- Evaluate model performance on test data\n",
    "- Compare results between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Device setup - use GPU if available\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Function\n",
    "\n",
    "This function loads our trained models from saved checkpoints. We're using two different architectures:\n",
    "1. MobileNetV3 Large - a lightweight model designed for mobile devices\n",
    "2. EfficientNet B0 - known for balancing accuracy and computational efficiency\n",
    "\n",
    "For both models, we've customized the classifier head to work with our binary classification task (RPW-trap vs NRPW-trap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model(model_name, path, num_classes=2):\n",
    "    if model_name == \"mobilenet\":\n",
    "        model = models.mobilenet_v3_large(weights=None)\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(model.classifier[0].in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    elif model_name == \"efficientnet\":\n",
    "        model = models.efficientnet_b0(weights=None)\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(model.classifier[1].in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "We apply the same transformations that were used during model training:\n",
    "1. Resize images to 224Ã—224 (standard input size for many CNN architectures)\n",
    "2. Convert to PyTorch tensors\n",
    "3. Normalize using ImageNet mean and standard deviation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Image preprocessing pipeline\n",
    "img_size = 224\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Function\n",
    "\n",
    "This function loads test images from their respective class folders and processes them using our transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_trap_images(folder_path, class_map):\n",
    "    images, labels = [], []\n",
    "    for class_name, label in class_map.items():\n",
    "        class_folder = os.path.join(folder_path, class_name)\n",
    "        img_paths = glob.glob(os.path.join(class_folder, \"*.*\"))\n",
    "        if not img_paths:\n",
    "            print(f\"âš ï¸ Skipping {class_name} â€” no images found.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(img_paths)} images in class {class_name}\")\n",
    "        for img_path in img_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = transform(img)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    return torch.stack(images), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "\n",
    "This function evaluates model performance using:\n",
    "1. Accuracy metric\n",
    "2. Confusion matrix visualization\n",
    "\n",
    "The confusion matrix helps us understand false positives and false negatives in our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, images, labels, class_names):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "    \n",
    "    print(f\"\\nAccuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Generate and plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(labels.cpu(), preds.cpu())\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return acc, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Evaluation\n",
    "\n",
    "Now we'll load the test data and evaluate both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Class definitions\n",
    "class_names = [\"NRPW-trap\", \"RPW-trap\"]\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "# Load test data\n",
    "test_root = \"test\"\n",
    "print(f\"Loading test images from {test_root}...\")\n",
    "images, labels = load_trap_images(test_root, class_to_idx)\n",
    "\n",
    "if images is None or len(images) == 0:\n",
    "    print(\"No images to evaluate. Please check your test data directory.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(images)} test images in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3 Evaluation\n",
    "\n",
    "MobileNetV3 is designed to be computationally efficient while maintaining good accuracy. Let's evaluate its performance on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Load and evaluate MobileNet\n",
    "    print(\"\\nðŸ” Loading MobileNetV3 model...\")\n",
    "    mobilenet = load_model(\"mobilenet\", \"saved_models/mobilenetv3_rpw.pth\")\n",
    "    print(\"MobileNetV3 Evaluation:\")\n",
    "    mobilenet_acc, mobilenet_cm = evaluate_model(mobilenet, images, labels, class_names)\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating MobileNetV3: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet Evaluation\n",
    "\n",
    "EfficientNet is known for its scaling method that uniformly scales network width, depth, and resolution. Let's evaluate EfficientNet B0 on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Load and evaluate EfficientNet\n",
    "    print(\"\\nðŸ” Loading EfficientNetB0 model...\")\n",
    "    efficientnet = load_model(\"efficientnet\", \"saved_models/efficientnetb0_rpw.pth\")\n",
    "    print(\"EfficientNetB0 Evaluation:\")\n",
    "    efficientnet_acc, efficientnet_cm = evaluate_model(efficientnet, images, labels, class_names)\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating EfficientNetB0: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's compare the performance of both models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Compare model accuracies\n",
    "    models = [\"MobileNetV3\", \"EfficientNetB0\"]\n",
    "    accuracies = [mobilenet_acc, efficientnet_acc]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(models, accuracies, color=['#3498db', '#2ecc71'])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print conclusion\n",
    "    better_model = \"MobileNetV3\" if mobilenet_acc > efficientnet_acc else \"EfficientNetB0\"\n",
    "    print(f\"\\nConclusion: {better_model} performed better on this test set with an accuracy of {max(accuracies):.4f}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error comparing models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we evaluated two deep learning models (MobileNetV3 and EfficientNetB0) on a palm weevil trap classification task. The evaluation metrics include:\n",
    "\n",
    "1. Overall accuracy\n",
    "2. Confusion matrix showing true positives, true negatives, false positives, and false negatives\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Based on the evaluation results, you might consider:\n",
    "- Using the better-performing model for deployment\n",
    "- Collecting more training data if accuracy is not satisfactory\n",
    "- Trying different model architectures or hyperparameters\n",
    "- Implementing ensemble methods to combine predictions from both models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
